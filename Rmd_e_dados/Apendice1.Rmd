---
title: 'Apêndice 1: detalhamento dos Métodos'
author: "MORI, Danilo P; LIMA, Renato AF; COUTINHO, Renato M; PRADO, Paulo I"
date: "14 de setembro de 2019"
output: 
  html_document:
    toc: true
    toc_depth: 5
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = FALSE, tidy = TRUE, include = TRUE, warning = FALSE,cache = TRUE)
```

```{r pacotes,warning=FALSE,message=FALSE,echo=FALSE}
library(doMC)
library(gridExtra)
library(tidyverse)
library(magrittr)
library(plyr)
library(DHARMa) # resíduos quantilicos
library(lme4) # pacote de criação dos modelos estatísticos
library(bbmle)
```

## Seleção dos levantamentos fitossociológicos

Filtros gerais:
i) effort >= 1ha; ii) DBH>=5cm; iii) ano dos dados >= 2000;

Filtros condicionais:
i) state %in% Rio de Janeira, Rio Grande do Sul -> ano dos dados >= 1990
ii) state %in% Bahia, Goiás, Mato Grosso do Sul -> ano dos dados >= 2000
iii) para as demais regiões ->  ano dos dados>= 1995
iv)  Exceções a esse esquema ocorreram quando trabalhos foram feitos antes do ano de 2000, mas foram realizados em grandes áreas de regiões protegidas (>1000ha) ou em antigos campi universitários, no qual foram incluídos na base de dados.

```{r dados e Primeiro filtro - DBH e amostra em bloco unico e disponibilidade da SAD}
df_references <- read.csv(file="/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/dados_brutos/references - TreeCo.csv",
                          as.is = TRUE,header =T, na.strings = c("","NA")) %>%
   select(refID,status,method,arrangement,samples,effort_ha,dbh_cutoff,SiteCode,lat,long,lat_municipio,long_municipio,lat_correct,long_correct,confiabilidade, Unidade_de_conservacao,Unidade_de_conservacao1,UC_area_ha,year_data,year,state,status_diagnostico,S, N,ordem)
# df_references %>% names
# df_references$status_diagnostico %>% unique
# filtro mais simples
df_references %<>% filter(method=="parcelas" & arrangement == "contiguous" & effort_ha>=1 & status == "yes" & grepl("ok*",status_diagnostico))
# df_references 
```

- 123 trabalhos ao todo no primeiro filtro

```{r Segundo filtro - fragmentos com area >= 1000ha}
# UC_class<- df_references$Unidade_de_conservacao %>% unique %>% .[c(1,3)]
UC_class<- df_references$Unidade_de_conservacao %>% unique %>% .[1]
df_simulacao <- df_references %>% filter(UC_area_ha >= 1000 & Unidade_de_conservacao %in% UC_class)
# df_simulacao
```

- 48 trabalhos feitos em universidade e UC de proteção integral e UC_area_ha >= 1000ha
- 42 trabalhos feitos em universidade e UC_area_ha >= 1000ha

```{r Filtros por estado e ano}
###    i) state %in% Rio de Janeira, Rio Grande do Sul -> ano dos dados >= 1990
# df_references$state %>% unique
# filter(df_references,state %in% c("RJ","RS") & year >= 1990)
df_simulacao %<>% rbind(.,filter(df_references,state %in% c("RJ","RS") & year >= 1990)) %>% unique # 17 ocorrências do filtro, 14 novas
df_simulacao %<>% rbind(.,filter(df_references,state %in% c("BA","GO","MS") & year >= 2000)) %>% unique # 22 ocorrências do filtro, 6 novas
df_simulacao %<>% rbind(.,filter(df_references,!(state %in% c("BA","GO","MS","RJ","RS")) & year >= 1995)) %>% unique # 87 ocorrências, 53 novas 
write.csv(df_simulacao,file="/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/df_referencia.csv",row.names = FALSE)
```

São 115 trabalhos pré-selecionados.

### Avaliação da abundances e dos .tif dos sites selecionados

- auditoria dos .tif

```{r}
#package
# install.packages("raster")
# install.packages("rgdal")
library(rgdal)
library(raster)


# data frame de referencia
df_ref <- read.csv(file="/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/df_referencia.csv",header = TRUE)
# data frame com as informações dos .tif
df_tif <- Sys.glob("/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/dados_brutos/*.tif") %>% # carregando
  gsub("/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/dados_brutos/","",.) %>% #removendo estrutura de pastas
  adply(.,1,function(x) unname( unlist( strsplit(x, "_", fixed = TRUE) ) )) %>% #dividindo a informação
  mutate(refID = gsub("ref","",V1),ordem=gsub(".tif","",V3)) %>% select(refID,ordem) 
df_tif$tif.name <- Sys.glob("/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/dados_brutos/*.tif") %>% #caminho dos .tif
  gsub("/home/danilo/Documentos/Doutorado/artigo_mestrado/1A.P_MOVER/dados_brutos/","",.)
#merge
class(df_tif$refID) <- class(df_ref$refID)
class(df_tif$ordem) <- class(df_ref$ordem)
df_ref %<>% inner_join(x=.,y=df_tif,by=c("refID","ordem")) 
# auditoria das imagens .tif selecionadas

for(i in 1:nrow(df_ref)){
  raster <- raster(df_ref$tif.name[1])
  mat_raster <- matrix(data = getValues(raster)/100, ncol=200)
  rotate <- function(a) t(apply(a, 2, rev))
  image(rotate(mat_raster),main=df_ref$SiteCode[i])
}

```


```{r}

```



## Relação entre Parâmetros

  Os modelos precisam de um parâmetro que controla a diversidade na metacomunidade e um que controla a dispersão. A relação entre os parâmetros de diversidade pode ser obtida a partir da igualdade proposta por por Vallade & Houchmandzadeh (2003) $\theta = \frac{U  (J_M - 1)}{1-U}$. Para isso aproximamos o número de indivíduos na metacomunidade como $J_M = A_{landscape} * DA_{obs}*p$.  
  
  
  O parâmetro m é a probabilidade de um evento de colonização na comunidade local ser por um propágulo da metacomunidade (Hubbel 2001). Uma vez que toda unidade de habitat apresenta igual probabilidade de colonização, podemos definir m como a média das probabilidades de cada sítio na comunidade local ser colonizada por um imigrante (eq 1; Chisholm & Lichstein 2009).

$$ eq 1: m = \frac{1}{A} \int \int_A m_{x,y} dxdy $$  

 Aproximamos as comunidades locais como áreas quadradas com lado L, que pode ser obtido por $L = \sqrt{10000\frac{J}{DA}}$. Assim podemos reescrever a equação 1 como:

$$ eq 2.a :   m = \left(\frac{1}{L} \int\limits_{-L/2}^{L/2} m_{x}(x)\mathrm{d}x \right)^2 $$

$$ eq 2.b :m_{x} = 1 - \int\limits_{-L/2}^{L/2} K(x-y) \mathrm{d}y $$ 

  Onde K é a função de dispersão. Na simulação coalescente a dispersão resulta do sorteio indepentende de duas distribuições de Laplace em eixos ortogonais centradas no sítio vago (figura X). Se consideramos a distribuição de Laplace como $K(x) = \frac{\alpha}{2} e^{-|\alpha x|}$ (idem para o eixo y), então:

$$eq3:m = (\frac{1-e^{-\alpha L}}{\alpha L})^2$$ 

Onde $\alpha = 1/b$, b é o parâmetro escalar da distribuição de Laplace que pode ser escrito em função do desvio-padrão da distribuição de Laplace (d) $b = d/ \sqrt{2}$. O desvio padrão corresponden à distância média de dispersão (Clark et al. 1999). Podemos reescrever a equação de m em funçao de d:

$$ eq4: m = d \frac{1 - e^{-\frac{\sqrt{2} L}{d}} }{\sqrt{2} L} $$ 

Com a equação 4 podemos calcular m a partir do desvio padrão da função de dispersão. Essa equação é válida para o processo de dispersão em paisagens homogêneas. Na simulação coalescente, uma vez que sorteamos um progenitor e este estaria presente em uma unidade de não habitat, o sorteio é refeito até que o progenitor esteja em uma unidade de habitat. Uma aproximação do efeito da fragmentação na simulação no m calculado a partir da equação 4 pode ser obtido por:

$$eq.5: m' = \frac{mp}{1 - (1-p)m} $$

Onde _p_ é a porcentagem de cobertura vegetal na paisagem. Caso seja necessário calcular d a partir de m, utilizamos o ramo principal da função W de Lambert ($W_{0}$):

$$ eq6 ANTIGA: d = \frac{\sqrt{2} L}{m W_{0}(- \frac{e^{-1/m}}{m} ) + 1} $$ 

$$ eq6 : d = \frac{\sqrt{2} L m}{m W_{0}(- \frac{e^{-1/m}}{m} ) + 1} $$ 

```{r figura X dispersao dos individuos segundo a funcao de dispersao utilizada, include=FALSE}
# calcular d a partir do m de BCI
# library(lamW)
# m <- 0.1
# J <- 213724
# A <- 50
# DA <- J/A
# L=sqrt((J/DA)*10000)
# l_cel <- 100/sqrt(DA)
# d <- sqrt(2)*L*m / (m * lambertW0(-exp(-1/m)/m) +1)
# 
# 
# 
# # simulação da chuva de propágulos assumindo dist Laplace para os dados de BCI
# library(rmutil)
# density <- 20852/50
# npoints <- 1e5
# d_ind_MA  <- 100/sqrt(density)
# b_laplace <- sigma / sqrt(2)
# X_laplace.CM <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA) 
# Y_laplace.CM <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA)
# X_laplace <- rlaplace(npoints, s=b_laplace)
# Y_laplace <- rlaplace(npoints, s=b_laplace)
# 
# 
# dist_laplace <- sqrt(X_laplace^2+Y_laplace^2)
# plot(x=X_laplace,y=Y_laplace)
# plot(density(dist_laplace))


```

Figura X. Chuva de propágulos pressuposta pela simulação coalescente. No primeiro quadro


## Parametrização da Dispersão
  
  Os sítios variam na densidade observada e portanto a distância entre os indivíudos na paisagem varia. Então optamos por parametrizar a dispersão pela proporção de propágulos que permanece até a vizinhança imediata da planta progenitora (k), padronizamos pela distância entre o centro de unidades de habitat adjacentes (l=100/sqrt(DA_obs)). Estimamos a distância média de dispersão necessária para obter determinado k.

```{r figura X dispersao dos individuos segundo a funcao de dispersao utilizada , include=FALSE}
# calcular d a partir do m de BCI
# 
# 
# 
# 
# 
# library(rmutil)
# density <- 20852/50
# npoints <- 1e5
# d_ind_MA  <- 100/sqrt(density)
# b_laplace <- sigma / sqrt(2)
# X_laplace.CM <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA) 
# Y_laplace.CM <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA)
# X_laplace <- rlaplace(npoints, s=b_laplace)
# Y_laplace <- rlaplace(npoints, s=b_laplace)
# 
# 
# dist_laplace <- sqrt(X_laplace^2+Y_laplace^2)
# plot(x=X_laplace,y=Y_laplace)
# plot(density(dist_laplace))
# 
```


  
```{r}
qkernel<- function(sigma, kernel, p, density=20852/50, npoints = 1e5){
    kernel <- match.arg(kernel, choices=c("normal","gaussian","laplace","uniform"))
    d_ind_MA  <- 100/sqrt(density)
    if(kernel=="laplace"){
        b_laplace <- sigma / sqrt(2)
        X_laplace <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA) 
        Y_laplace <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA)
        dist_laplace <- sqrt(X_laplace^2+Y_laplace^2)
        result <- quantile(dist_laplace, p)
    }
    if(kernel=="normal"|kernel=="gaussian"){
        b_norm <- sigma 
        X_norm <- d_ind_MA * round(rnorm(npoints, sd=b_norm) / d_ind_MA)
        Y_norm <- d_ind_MA * round(rnorm(npoints, sd=b_norm) / d_ind_MA)
        dist_norm <- sqrt(X_norm^2+Y_norm^2)
        result <- quantile(dist_norm, p)
    }
    if(kernel=="uniform"){
        b_unif <- sigma/2
        X_unif <- d_ind_MA * round(runif(npoints, min = -b_unif, max = b_unif) / d_ind_MA)
        Y_unif <- d_ind_MA * round(runif(npoints, min = -b_unif, max = b_unif) / d_ind_MA)
        dist_unif <- sqrt(X_unif^2+Y_unif^2)
        result <- quantile(dist_unif, p)
    }
    return(unname(result))
}

sigkernel <- function(kernel, p, distance, density=20852/50,
                      npoints =1e5, sigma.min = 1, sigma.max= 100){
    f1 <- function(x) distance - qkernel(x, kernel, p, density, npoints)
    uniroot( f1 , lower = sigma.min, upper = sigma.max)
}
```
  




A chuva de propágulos pode ser entendida como o produto da fecundidade e função de dispersão (Clark et al. 1999). Por conta do pressuposto da equivalência funcional todos os indivíduos produzem o mesmo número de propágulos por unidade de tempo (Hubbell 2001), assim, podemos simular cenários de limitação à dispersão em função da porcentagem de propágulos que permace até determinada distância da planta progenitora. Dessa maneira, não precisamos definir a dispersão em termos de distância per se mas em termos de porcentagem de indivíduos que permanecem na área imediata da planta progenitora. Para isso é necessário estabelecer uma distância padrão da planta progenitora e estimar ou definir a porcentagem de indivíduos que se mantêm até esta distância padrão. Como distância padronizamos $l_{cel}$, assim, cada paisagem possui uma distância padrão que depende da densidade observada de indivíduos naquela paisagem [REESCREVER]. Podemos estimar qual a porcentagem de propágulos até a distância padrão que um determinado sd gera, partindo de um m (eqn 2); ou podemos informar a priori quais as porcentagens de interesse e estimar o sd necessário para gerar tais porcentagens. Na simulação coalescente, utilizamos 12 valores de porcentagem para simular os cenários de limitação à dispersão: 99%, seq(95,50,by=-5)% e 25%. Apesar da simulação coalescente ser bem eficiente e permitir simular paisagens infinitas, funções de dispersão que apresentam dispersão muito elevadas são computacionalmente muito onerosas (Rosindell et al. 2008) e apresentariam pouco realismo biológico (REFERÊNCIA), logo, não utilizamos porcentagens muito baixas (e.g.<1%).

Para estimar o sd necessário para gerar uma determinada porcentagem de propágulos até a distância padronizada, desenvolvemos uma função no ambiente de programação R (R language team). A seguir o código utilizado nessa função, note que a função permite utilizar 3 distribuições de probabilidade (uniforme, normal e Laplace), contudo utilizamos apenas a distribuição Laplace.


## Análise Estatística Completa

```{r preparacao dos dados,warning=FALSE,message=FALSE, include=FALSE}
### leitura ###
df_resultados <- readr::read_csv("/home/danilo/Documentos/Doutorado/artigo_mestrado/simulacao/resultados/df_resultados.csv")
# names(df_resultados)[1] <- "Site"
# df_resultados %>% str
### padronização ###
## fatores
# df_resultados$Site <- factor(df_resultados$Site)
# df_resultados$k <- factor(df_resultados$k,levels=unique(df_resultados$k))
df_resultados$k.0 = as.numeric(as.character(df_resultados$k))
df_resultados$MN <- factor(df_resultados$MN,levels = c("EE","EI"))
# df_resultados$k %>% contrasts()
# df_resultados$MN %>% contrasts()

### z score ### 
f_z <- function(x){
  m <- base::mean(x,na.rm=TRUE)
  sd <- sd(x,na.rm=TRUE)
  output <- (x-m)/sd
  return(output)
} 
# names(df_resultados)
df_resultados.z <- as.data.frame(apply(df_resultados[c("p","J","S","k.0")],2,f_z))
names(df_resultados.z) <- sapply(names(df_resultados.z), function(x) paste(gsub(".0","",x),".z",sep=""))
df_resultados %<>% cbind(.,df_resultados.z)
### Summary ###
# df_resultados %>% head

### para auditoria ### remover depois 
df_resultados.rep <- read.csv("/home/danilo/Documentos/Doutorado/artigo_mestrado/simulacao/resultados/df_replicas.csv")
df_resultados.rep %<>% left_join(x=.,y=df_resultados[,c("SiteCode","J","S")],by="SiteCode")
```

### Descrição dos Levantamentos Selecionados

```{r figura 1, fig.width=8, fig.height=5, fig.align="center", include=FALSE}
df_plot <- df_resultados %>% dplyr::filter(k=="0.99" & MN=="EE") %>% dplyr::select(Site, p, S,p.z,S.z) %>% unique

# avaliando as curvas pelo quantiles
# df_plot %<>% mutate(terceiro_quantil = ifelse(p>quantile(df_plot$p,probs=0.75),">_3oQ","<_3oQ")) 
l_p <- vector("list",6)
l_p[[1]] <- ggplot(df_plot, aes(x=p)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = c(quantile(df_plot$p,probs = c(0.25,0.50,0.75))),color="red")
  # geom_density()
l_p[[2]] <- ggplot(df_plot, aes(x=p.z)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = c(quantile(df_plot$p.z,probs = c(0.25,0.50,0.75))),color="red")
  # geom_density()
l_p[[3]] <- ggplot(df_plot, aes(x=S)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = quantile(df_plot$S,probs = c(0.25,0.50,0.75)),color="red")
l_p[[4]] <- ggplot(df_plot, aes(x=S.z)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = c(quantile(df_plot$S.z,probs = c(0.25,0.50,0.75))),color="red")
l_p[[5]] <- ggplot(df_plot, aes(x=p,y=S)) +
  geom_hline(yintercept = quantile(df_plot$S,probs = c(0.25,0.50,0.75)),color="red") +
  geom_vline(xintercept = quantile(df_plot$p,probs = c(0.25,0.50,0.75)),color="red") +
  geom_point() + 
  geom_smooth(method="lm")
l_p[[6]] <- ggplot(df_plot, aes(x=p.z,y=S.z)) +
  geom_hline(yintercept = quantile(df_plot$S.z,probs = c(0.25,0.50,0.75)),color="red") +
  geom_vline(xintercept = quantile(df_plot$p.z,probs = c(0.25,0.50,0.75)),color="red") +
  geom_point() + 
  geom_smooth(method="lm")
# do.call("grid.arrange",c(l_p,ncol=3))
grid.arrange(l_p[[1]],l_p[[2]],l_p[[3]],l_p[[4]],l_p[[5]],l_p[[6]],
             layout_matrix = rbind(c(1,1,3,3,5,5),
                                   c(2,2,4,4,6,6) )
             )

```

Figura 1. Na primeira linha há as variáveis na escala padrão; na segunda linha as variáveis após transformação Z (centra a média em zero e desloca a variação para o centro da distribuição REVISÃO). As linhas em vermelho equivalem ao quantil de 0.25%, 0.50% e 0.75% da amostra. S = riqueza observada; p = proporção de cobertura vegetal na paisagem

  A proporção de cobertura vegetal variou de 0.0074 até 1, o quantil de 25% é de 0.2916, a média é 0.6727 e o quantil de 75% é de 0.9216 (figura 1). A riqueza observada variou de 26 até 230, o quantil de 25% é de 73.75, a média é 105.85, e o quantil de 75% é 134 (figura 1). Há um vies na amostra que apresenta mais trabalhos em paisagens com alta cobertura vegetal do que em baixas, por exemplo, o primeiro 1/4 da amostra está entre 0.00 e 0.30, enquanto o último 1/4 está comprimido entre 0.92 e 1 (figura 1). A riqueza observada apresenta um outro padrão com uma tendência central e uma assimetria para a esquerda [REVISAR]: 50% da amostra está entre 73 e 134 com média e mediana próximos de 100; o primeiro 1/4 da amostra está entre 26 e 73 enquanto o último 1/4 varia entre 134 e 230, o range do 1o quarto equivale à metade do range do último quarto da amostra. Há certa covariação entre p e S: o último quarto de p varia acima do quantil de 25%; enquanto o primeiro quarto de p varia até a mediana de S. Porém os 50% centrais de cada variável estão representadas em todo o gradiente de variação da outra, e.g., entre o quantil de 25% e 75% de p observamos S que varia desde de valores inferiores à 50 até superiores à 200; e um padrão se observa para S. Para realizar a análise estatística aplicamos a transformaçaõ Z em p e S. A transformação Z centraliza no zero a média da distribuição e converte da escala da variável para a de desvio-padrões; dessa forma torna-se mais direta a interpretação de modelos lineares generalizados hierarquicos (REF 2006). Essa transformação move a variação para a região central da distribuição mantendo a relação geral entre as observações (figura 1). Não há motivos a priori para pensar que a predição dos modelos pode ser influênciada pela covariação entre p e S. [DÚVIDA] Paulo, lembro que discutimos sobre a relação entre teste frequentista e o efeito de S * p; me recordo de algo como que ao utilizar o p-valor estariamos de alguma forma ponderando isso [DÚVIDA].

### Congruência entre SAD observada e predita

#### Auditoria das Replicas

  Espera-se que quanto maior a estatística D do teste de Kolmogorov-Smirnov menor o p-valor observado. Não esperamos observar relação entre p-valor, J e S. Segue avaliação destas espectativas

```{r auditoria teste KS,fig.height=4,fig.width=10}
l_p <- vector("list",3)
l_p[[1]] <- ggplot(df_resultados.rep,aes(x=KS.D,y=KS.p)) + geom_point()
l_p[[2]] <- ggplot(df_resultados.rep,aes(x=J,y=KS.p)) + geom_point() + labs(y="") + theme(axis.text.y = element_blank(), axis.ticks.x = element_blank())
l_p[[3]] <- ggplot(df_resultados.rep,aes(x=S,y=KS.p)) + geom_point() + labs(y="") + theme(axis.text.y = element_blank(), axis.ticks.x = element_blank())
do.call("grid.arrange",c(l_p,ncol=3))
```

Figura 2. o p-valor obtido do teste KS (eixo y) e a maior distância entre os vetores de abundância (KS), tamanho da amostra (J) e riqueza observada (S).

O método parece estar adequado: i) KS.p e KS.D apresentam uma relação inversa; ii) não há covariação entre KS.p com J e S.


#### Modelo Estatístico 

  Consideramos que um modelo neutro não foi refutado quando o p-valor for maior ou igual à 5%. Contabilizamos o número de predições não refutadas (Goodness-of-fit) e modelamos a probabilidade de uma predição não ser refutada usando um modelo logito. Agrupamos os dados pelo Sítio de observação (Site). É possível agrupar os dados considerando um intercepto por sítio (1|Site); 1 intercepto por modelo neutro (MN|Site); ou com 1 intercepto e 1 inclinação para k por modelo neutro (k*MN|Site). Na última opção k precisa ser interpretado como variável contínua. Um protocolo de seleção de modelos hierarquicos pode ser encontrado em Zuur et al. 2009 onde se recomenda comparar formas alternativas de agrupar os dados a partir do modelo cheio da relação entre as preditoras. O modelo cheio proposto foi com a interação de terceiro grau entre as preditoras p, k e MN. Comparamos todas as combinações possíveis por verossimilhança [DÚVIDA] se entendo corretamente, os parametros da estrutura aleatória são estimados pelo R2 e os estrutura fixa por algo similar à verossimilhança; então precisa utilizar o parâmetro REML=TRUE [DÚVIDA]

__Tabela 1__ Qual o melhor modelo cheio?
  
```{r tabela 1 comparacao da estrutura aleatoria de GOF, include=FALSE}
l_md.cheio <- vector("list",5)
names(l_md.cheio) <- c("k.z 1|Site","k.z MN|Site","k.z k.z*MN|Site","k.f 1|Site","k.f MN|Site")
l_md.cheio[[1]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k.0.z * MN + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.cheio[[2]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k.0.z * MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.cheio[[3]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k.0.z * MN + (k.0.z*MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.cheio[[4]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k * MN + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.cheio[[5]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k * MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
AICctab(l_md.cheio,weights=T)
```

O modelo estatístico com k como fator e agrupamento dos dados como MN|Site foi o único plausível.

__Tabela 2__ Qual a relação entre as variáveis é mais plausível?

```{r GOF selecao de modelos, include=FALSE}
l_md.selecao <- vector("list",19)
names(l_md.selecao) <- c("p*k*MN",# modelo cheio
                         "p*k*MN-p:k:MN", #MC - 3a ordem
                         "p*(k+MN)","k*(p+MN)","MN*(p+k)", #2 interações
                         "p*k+MN","p*MN+k","k*MN+p", #1 interação + preditor
                         "p*k","p*MN","k*MN", #1 interação
                         "p+k+MN",#aditivo 3
                         "p+k","p+MN","k+MN", #aditivo 2 
                         "p","k","MN", #preditor isolado
                         "1") #nulo
#modelo cheio
l_md.selecao[[1]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k * MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#modelo cheio - interação 3a ordem
l_md.selecao[[2]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k * MN - p.z:k:MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#2 interações
l_md.selecao[[3]] <- glmer(cbind(GOF,100-GOF) ~ p.z * (k + MN) + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[4]] <- glmer(cbind(GOF,100-GOF) ~ k * (p.z + MN) + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[5]] <- glmer(cbind(GOF,100-GOF) ~ MN * (p.z + k) + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#1 interação + preditor
l_md.selecao[[6]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k + MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[7]] <- glmer(cbind(GOF,100-GOF) ~ p.z * MN + k + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[8]] <- glmer(cbind(GOF,100-GOF) ~ k * MN + p.z + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#1 interação
l_md.selecao[[9]] <- glmer(cbind(GOF,100-GOF) ~ p.z * k + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[10]] <- glmer(cbind(GOF,100-GOF) ~ p.z * MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[11]] <- glmer(cbind(GOF,100-GOF) ~ k * MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#aditivo 3
l_md.selecao[[12]] <- glmer(cbind(GOF,100-GOF) ~ p.z + k + MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#aditivo 2
l_md.selecao[[13]] <- glmer(cbind(GOF,100-GOF) ~ p.z + k + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[14]] <- glmer(cbind(GOF,100-GOF) ~ p.z + MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[15]] <- glmer(cbind(GOF,100-GOF) ~ k + MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
#aditivo 1
l_md.selecao[[16]] <- glmer(cbind(GOF,100-GOF) ~ p.z + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[17]] <- glmer(cbind(GOF,100-GOF) ~ k + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
l_md.selecao[[18]] <- glmer(cbind(GOF,100-GOF) ~ MN + (MN|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
# nulo
l_md.selecao[[19]] <- glmer(cbind(GOF,100-GOF) ~ 1 + (1|Site), family = "binomial",data=df_resultados,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
AICctab(l_md.selecao,weights=T)
```

O único modelo plaúsivel é aquele que inclui a interação de terceiro grau. Vamos avaliar os resíduos quantílicos utilizando o pacote DHARMa (REF). Se o modelo estatístico está fazendo um ajuste adequado então a distribuição dos resíduos quantílicos deve ser próximo a de uma distribuição normal (REF). 

```{r res quant l_md.selecao, include=FALSE}
plot(simulateResiduals(l_md.selecao[["p*k*MN"]],1000))
```

Figura 2. Resíduos quantílicos do modelo mais plaúsivel [aquele que inclui p:k:MN], para detalhes ver documentação da função simulateResiduals.

A distribuição dos resíduos quantílicos do modelo mais plausível não apresenta boa aderência à uniformidade.


## Anexo:

### Imagens das matrizes de paisagem


```{r matrizees de paisagem, include=FALSE}
path_paisagens <- Sys.glob("/home/danilo/Documentos/Doutorado/artigo_mestrado/simulacao/*.txt")
df_simulacao <- map_df(Sys.glob("~/Documentos/Doutorado/artigo_mestrado/simulacao/U/*.csv"),read.csv) 
df_simulacao %<>% dplyr::select(SiteCode,p,J,S,DA,txt.file) %>% unique %>% 
  left_join(x=.,
            y=data.frame(txt.file=gsub("/home/danilo/Documentos/Doutorado/artigo_mestrado/simulacao/","",path_paisagens),path_paisagens),
            by = "txt.file")
df_simulacao$path_paisagens %<>% as.character()

## matriz de paisagem

# f_plot <- function(X){
#   mat_paisagem <- read.table(X$path_paisagens,header = FALSE) %>% as.matrix()
#   rotate <- function(a) t(apply(a, 2, rev))
#   image(rotate(mat_paisagem),main=X$txt.file)
# }
# par(mfrow=c(20,4))
# registerDoMC(2)
# a_ply(df_simulacao,1,f_plot,.parallel = TRUE)


for(i in 1:nrow(df_simulacao)){
  X <- df_simulacao[i,]
  mat_paisagem <- read.table(X$path_paisagens,header = FALSE) %>% as.matrix()
  rotate <- function(a) t(apply(a, 2, rev))
  image(rotate(mat_paisagem),main=paste0(X$SiteCode, " p=",X$p," J=",X$J," S=",X$S))
}

```



