---
title: "Artigo mestrado -  criação dos dados"
author: "Mori, Danilo Pereira"
date: "14 de fevereiro de 2018"
output: pdf_document
---

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(echo=T, warning=FALSE, message=FALSE, cache = TRUE, tidy = TRUE, include = TRUE)
```

```{r global packages and data, echo=F, message=FALSE, warning=FALSE}
library(tidyverse)
library(sads)
library(gridExtra)
library(ggplot2)
library(rmutil)
library(untb)
library(doMC)
library(GUILDS)
library(magrittr)
library(plyr)
library(dplyr)

# df_ad <- read.table("/home/danilo/Documentos/Doutorado/dados/df_dados.txt", sep = "\t",header = TRUE)
# names(df_ad)[13] <- "sd_k"
# df_SAD.obs <- read.table("/home/danilo/Documentos/Doutorado/dados/df_SADs-obs.txt", sep = "\t", header = TRUE)
```

<!--
Estrutura do arquivo:
- Prólogo  
- seleção dos sítios  
- criação da matriz de paisagem
- modelo neutro clássico  
  i) ajuste dos parâmetros do modelo de Etienne 2005 às SADs observadas  
  ii) simulação das SADs a partir dos parâmetros ajustados
  iii) comparação das SADs usando teste KS e sintese dos dados 
- modelo neutro coalescente (Rosindell et al. 2008)  
  i) seleção dos percentis de interesse da função de dispersão  
  ii) estimativa da taxa de extinção necessária para gerar a riqueza observada (U)  
  iii) simulação das SADs a partir do U estimado  
  iv) comparação das SADs usando teste KS e síntese dos dados
- organização dos dados para modelagem estatística  
- parâmetros modelo EI a partir das SADs{EE}
### Prólogo ###

  O objetivo geral dos métodos é gerar distribuições de abundância de espécies (SAD) segundo dois modelos neutros e comparar com as SADs observadas em um gradiente de cobertura vegetal. O primeiro modelo neutro é uma derivação do modelo clássico de Hubbell 2001, onde considera-se uma dinâmica ecológica ocorrendo em 'campo médio', ou seja, com o espaço implicito (Etienne et al. 2005); o segundo modelo neutro considera a distância e posição dos indivíduos na comunidade local e paisagem, permitindo simular diferentes cenários de limitação à dispersão (Rosindell et al. 2008). O fluxo de trabalho é similar em ambos os casos: ajusta-se/estima-se os parâmetros dos modelos a partir das SADs observadas, então simula-se 100 SADs réplicas segundo os respectivos modelos e compara-se SADs observadas e simuladas usando o teste de Kolmogorov-Smirnov. Em cada caso, contabiliza-se o número de réplicas em que não é possível refutar a hipótese de que as SADs são amostras de uma mesma distribuição teórica.

### Seleção dos Sítios ###
  
  Obtemos as distribuições de abundância de espécies (SAD) da base de dados TreeCo. As SADs selecionadas são oriundas de trabalhos fitossociológicas realizados na Mata Atlântica com amostragem contígua (em bloco único) de pelo menos 1 hectare (>= 1ha) e com a coordenada central da amostragem disponível e precisa. Além disso, foram descartados os sítios em que não foi possível obter um recorte de paisagem adequado à data de meu mestrado. Outro critério foi a data da amostragem, como as imagens de satélite foram tiradas em 2000 estabeleceu-se uma janela de tempo aceitável para que a paisagem da imagem fosse suficientemente similar àquela da época do estudo; para mais detalhes procurar email de Lima. 
  Por hora vou utilizar os sítios em que as simulações coalescentes (modelo neutro com espaço explicito) já haviam sido rodadas. Se for necessário/possível acrescentar sítios aos dados, a janela de código a seguir será expandida, por hora vou apenas selecionar aqueles sítios que já foram trabalhados.
--> 

Como os dados para a simulação coalescente (modelo EE) já foram obtidos no passado, aqui vou apenas simular as SADs para o modelo de espaço implícito (EI). Primeiro seleciono as SADs observadas filtradas e então ajusto o modelo de Etienne 2005 a elas e obtenho theta e m. Então simulo 100 SADs para cada conjunto de parâmetros e comparo cada réplica neutra com a respectiva SAD observada usando o teste de Kolmogorov-Smirnov e contabilizo o número de réplicas onde não foi possível refutar a hipótese nula na variável GOF.

Após a rodada de simulações e comparação realizo a conversão de parâmetros entre EI e EE. Para o modelo EI estimo U, d e seu correspondente percentil de propágulos. Para EE estimo theta e m. Na sessão da conversão de parâmetros eu descrevo melhor como realizei as conversões. Gostaria que vocês confirmassem se: a) fiz a correspondência correta entre 'd', a distância média de dispersão, e o sigma da distribuição de LaPlace; e b) se calculei corretamente o percentil correspondente ao sigma - linhas [252;267] e [296;267].

  
```{r selecao dos sitios e preparacao das SADs}
SiteCodes <- as.factor(df_par.Etienne$SiteCode) %>% levels # sitecodes de trabalho
# convertendo df_SAD.obs em lista para facilitar as operações. 
l_SAD.obs <- df_SAD.obs[,1:3] %>% filter(SiteCode %in% SiteCodes) %>% droplevels() %>% split(x=.,f=.$SiteCode) # lista de SADs obs
```

<!--
### Criação da Matriz de Paisagem ###

  Selecionado os sítios de trabalho é necessário criar a matriz de paisagem onde o modelo neutro de espaço explícito simula a dinâmica ecológica. A matriz de paisagem é oriunda de uma foto de satélite com resolução de 30X30m onde cada pixel representa a cobertura vegetal na área (podendo variar de 0-100). Quando um pixel >= 70 então consideramos que é habitat e quando não for é considerado não habitat.


```{r}

```
-->


### Modelo neutro de espaço implícito ###

Anteriormente haviamos usado o modelo de Volkov et al. 2003, contudo, para ficar coerente com o modelo de espaço explícito vamos utilizar a formula de amostragem desenvolvida por Etienne 2005, onde ele a deriva utilizando a abordagem coalescente. Para isso utilizo funções do pacote 'untb' (que precisam da biblioteca PARI/GD), que estimam os parâmetros do modelo, e uma função do pacote 'GUILDS', que simula SADs segundo o mesmo modelo.


```{r Simulacao Etienne 2005}
## Ajustando as SADs ao modelo de Etienne e armazendo os coefs em um data frame ##
# SdT: i) estimar os parâmetros usando a função optimal.params; ii) gerar lista com 100 SADs para cada; iii) comparar cada conjunto de réplicas com a SAD original usando o teste de Kolmogorov-Smirnov

## Funcao para estimar os parâmetros do modelo neutro clássico a partir da formula de Etienne 2005 ##
fitetienne <- function(SAD_){
  SAD_count <- untb::count(SAD_[,"N"])
  SAD_log.kda <- logkda.pari(SAD_count)
  par_etienne <- optimal.params(SAD_count, log.kda = SAD_log.kda)
  return(par_etienne)
}

# Estimando os parâmetros a partir da lista de SADs e armazenar os valores em um data frame #
registerDoMC(3)
df_par.Etienne <- ldply(l_SAD.obs, fitetienne, .parallel = TRUE)
df_par.Etienne %<>% mutate(J = sapply(l_SAD.obs,function(X) sum(X$N)),#tamanho do amostra
                           S = sapply(l_SAD.obs,function(X) length(X$N)), #riqueza observada
                           I = m * (J-1)/(1-m)) #convertendo parâmetros

## Simulando 100 SADs para cada conjunto de parâmetros ##
f_SAD.Etienne <- function(i,df_=df_par.Etienne,replicas=100){
  SAD.replicas <- replicate(replicas,
                            {generate.ESF(theta = df_[i,"theta_"],
                                          I = df_[i,"I"],
                                          J = df_[i,"J"])
                            }
  )
}

# Lista com as SADs simuladas #
registerDoMC(3)
l_SAD.Etienne <- llply(.data = as.list(1:nrow(df_par.Etienne)),
                       .fun = f_SAD.Etienne, .parallel = TRUE)
```


<!--
Ideia geral dos métodos:

i) estabelecer valores do parâmetro de dispersão a priori;
ii) estimar taxa de especiação necessária para manter a riqueza no equilíbrio a partir do método de MNEE
iii) simular SADs neutras segundo MNEI e MNEE a partir dos mesmo parâmetros
-->

```{r Simulacao Etienne 2005 EE - EI}
################################################################################################################
################################################ par_EE -> MNEI ################################################
################################################################################################################

# Danilo 05jun2018: Vou modificar o data frame df_par.Etienne para os valores de m_ segundo os d's da parametrização atual(k:[0.99,0.25]) e theta segundo U_EE.
# Futuramente vou adicionar valores de k.

## Dados ##

############ par_EE ############
# dados para gerar as SAD_EI a partir de par_EE #
df_par.Etienne <- read.table(file="/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_resultados_conv.txt", header=T, sep="\t",as.is = T)
## arrumando fatores ##
df_par.Etienne$k <- factor(df_par.Etienne$k,
                           levels=df_par.Etienne$k[c(12:1,13)])
df_par.Etienne$SiteCode <- factor(df_par.Etienne$SiteCode)
## criando variáveis, filtrando e arrumando o df ##
df_par.Etienne %<>% mutate(d = sd_k / sqrt(2), d_ = sd_k_ / sqrt(2))
df_par.Etienne %<>% filter(k!="EI") %>% dplyr::select(SiteCode, k, GOF, p, J, S, DA, d,theta_,m_) %>% mutate(I = m_ * (J-1)/(1-m_)) %>% droplevels()
# df_par.Etienne %>% summary
## nest o df ## 
df_resultados <- df_par.Etienne %>% group_by(SiteCode) %>% nest

############ SAD_obs ############
sitecodes <- df_resultados$SiteCode %>% unique %>% as.character()
df_SAD.obs <- read.table("/home/danilo/Documentos/Doutorado/dados/df_SADs-obs.txt", sep = "\t", header = TRUE) %>% 
  dplyr::select(SiteCode,N) %>% filter(SiteCode %in% sitecodes) %>% group_by(SiteCode) %>% nest() %>% dplyr::rename(SAD_obs = data)

############ inner_joiin ############

df_resultados %<>% inner_join(x=.,y=df_SAD.obs,by="SiteCode")

## Função ##

############ SAD_rep ############
f_SAD.Etienne <- function(df_par){
  l_SAD.rep <- dlply(df_par,"k",function(X) replicate(100,generate.ESF(theta = X$theta_, I = X$I, J = X$J)))
}
#modificar para uma versão que parametriza, talvez utilizar um llply %>% unlist()
df_resultados %<>% mutate(SAD_rep = map(data,f_SAD.Etienne))

save(df_resultados,file = "/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_resultados.Rdata")
############ GOF, S e estatisticas resumo ############
df_resultados <- load(file = "/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_resultados.Rdata")

### teste ###
df_teste <- df_resultados$data[[1]]
SAD_obs.teste <- df_resultados$SAD_obs[[1]] %>% .$N
l_teste <- dlply(df_teste,"k",function(X) replicate(100,generate.ESF(theta = X$theta_, I = X$I, J = X$J)))
# l_teste %>% names()

f_ks.test0 <- function(SAD.obs = SAD_obs.teste, SAD.sim){
    a <- suppressWarnings(ks.test(x = SAD.obs,
                                  y = SAD.sim))
    a <- data.frame(KS.D = a$statistic, KS.p = a$p.value)
}
df_teste2 <- ldply(l_teste,function(X) ldply(X,function(Y) f_ks.test0(SAD.sim = Y)))
df_teste2 %<>% ddply(.,"k",function(x) nrow(x[x$KS.p >= 0.05,]))


f_GOF.Etienne <- function(par,SAD.obs){
  # dados #
  df_par <- par
  SAD__obs <- SAD.obs$N
  # SADs replicas #
  l_SAD.rep <- dlply(df_par,"k",function(X) replicate(100,generate.ESF(theta = X$theta_, I = X$I, J = X$J)))
  # ks teste #
  f_ks.test0 <- function(SAD.obs = SAD__obs, SAD.sim){
      a <- suppressWarnings(ks.test(x = SAD.obs,
                                    y = SAD.sim))
      a <- data.frame(KS.D = a$statistic, KS.p = a$p.value)
  }
  # armazenando #
  df_return <- ldply(l_SAD.rep,function(X) ldply(X,function(Y) f_ks.test0(SAD.sim = Y)))
  df_return  <-  inner_join(x=ddply(df_return,"k",function(x) nrow(x[x$KS.p >= 0.05,])),
                            y=ddply(df_return,"k",summarise, KS.D_mean = mean(KS.D),KS.D_var = var(KS.D), KS.p_mean = mean(KS.p), KS.p_var = var(KS.p)),
                            by = "k")
return(df_return)  
}

ddply(df_resultados,"SiteCode",function(A) f_GOF.Etienne(par = A$data, SAD.obs = A$SAD_obs) )

for(i in 1:nrow(df_resultados)){
  # dados #
  df_par <- df_resultados$data[[i]]
  SAD__obs <- df_resultados$SAD_obs[[i]]
  # SADs replicas #
  l_SAD.rep <- dlply(df_par,"k",function(X) replicate(100,generate.ESF(theta = X$theta_, I = X$I, J = X$J)))
  # ks teste #
  f_ks.test0 <- function(SAD.obs = SAD__obs, SAD.sim){
      a <- suppressWarnings(ks.test(x = SAD.obs,
                                    y = SAD.sim))
      a <- data.frame(KS.D = a$statistic, KS.p = a$p.value)
  }
  # armazenando #
  df_return <- ldply(l_SAD.rep,function(X) ldply(X,function(Y) f_ks.test0(SAD.sim = Y)))
  df_return  <-  inner_join(x=ddply(df_return,"k",function(x) nrow(x[x$KS.p >= 0.05,])),
                            y=ddply(df_return,"k",summarise, KS.D_mean = mean(KS.D),KS.D_var = var(KS.D), KS.p_mean = mean(KS.p), KS.p_var = var(KS.p)),
                            by = "k")
return(df_return)  
}

l_teste[[1]][[100]] %>% length()
l_SAD.rep[[1]][[1]] %>% length()
```

### Comparação das SADs simuladas e observadas utilizando o teste KS ###

```{r comparacao teste KS}
# função que utiliza o teste não parâmetrico bicaudal de Kolmogorov-Smirnov
# hipótese nula: os dois vetores de abundância são amostras de uma mesma distribuição teórica
f_ks.Etienne <- function(i, SAD_obs = l_SAD.obs, SAD_sim = l_SAD.Etienne){
  f_ks.test0 <- function(SAD.obs, SAD.sim){
    a <- suppressWarnings(ks.test(x = SAD.obs,
                                  y = SAD.sim))
    a <- data.frame(KS.D = a$statistic, KS.p = a$p.value)
  }
    ks_stat <- ldply(SAD_sim[[i]],function(A) f_ks.test0(SAD.obs = SAD_obs[[i]][,"N"], SAD.sim = A))
}

# lista onde cada elemento é um df com o valor da estatística KS e o p valor associado da comparação entre a SAD obs e as réplicas neutras #
l_df.KS.Etienne <- llply(.data = as.list(1:length(l_SAD.obs)),
                         .fun = f_ks.Etienne, .parallel = TRUE)
## Sintetizando os dados ##
df_par.Etienne %<>% cbind(., 
                          ldply(l_df.KS.Etienne, function(x) nrow(x[x$KS.p >= 0.05,])), # GOF
                          ldply(l_df.KS.Etienne, summarise, KS.D_mean = mean(KS.D),KS.D_var = var(KS.D), KS.p_mean = mean(KS.p), KS.p_var = var(KS.p))) # estat resumo
names(df_par.Etienne)[c(1,7)] <- c("SiteCode","GOF")
```


### Auditoria dos dados ###
- Avaliação visual da autocoerência dos dados:
  
```{r auditoria dados Etienne}
# save.image(file = "/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/dados_completo.Rdata")
# load("/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/dados_completo.Rdata")
# x11()
car::scatterplotMatrix(~ GOF + KS.p_mean + KS.D_mean, 
                       reg.line = "", smoother = "" ,
                       data=df_par.Etienne, main = "Avaliação Visual de GOF e estats resumo modelo EI")
```

figura 1. GOF - bondade de ajuste (número de SADs réplicas onde não foi possível refutar a hipótese nula para cada conjunto de parâmetros); KS.p_mean - média dos 'p-values' de cada bateria de simulações; KS.D_mean - média da estatística de interesse D do teste de Kolmogorov-Smirnov, considerando uma distribuição nula de D quanto menor for D menor a probabilidade de se refutar a hipótese nula. 

Diria que está autocoerente: GOF é proporcional à KS.p_mean e KS.p_mean é inversamente proporcional à KS.D_mean. 

<!--
### Modelo neutro coalescente ###

#### estimativa dos sigmas ####

```{r}

```

#### estimativa dos U ####

```{r}

```

#### simulação das SADs e teste KS ####

```{r}

```

#### Auditoria dos Dados ####
-->


### Conversão dos parâmetros ###

__Objetivo:__ Comparar os dois métodos de conversão de parâmetro de migração entre um modelo neutro de campo médio (Etienne 2005) e um modelo neutro de espaço explícito (Rosindell et al. 2008) a partir de deduções da eq 1 apresentada por Chisholm & Lichstein (2009). O primeiro método é desenvolvido pelos autores anteriores e trata-se de uma aproximação (eq2 Chisholm & Lichstein 2009); o segundo método foi deduzido a partir da eq 1 e aproveitou das particularidades de nossas simulações para deduzir uma equação. Ambos métodos relacionam a probabilidade de um indivíduo de fora da comunidade colonizar uma unidade de habitat na comunidade local por evento de morte ('m') com a distância média de dispersão dos indivíduos na paisagem ('d'). 

O modelo neutro espacialmente explícito tem as seguintes características: i) utilizamos apenas áreas amostradas contíguas que aproximamos como quadrados; ii) a distribuição de probabilidade subjacente a função de dispersão é Laplace e parametrizamos a partir do desvio-padrão; iii) a área foi escrita em função de J e DA, número de indivíduos e densidade(ind/ha) observada na amostra, respectivamente.


### Contexto ###

Chisholm & Lichstein (2009) (doravante C&L09) estabeleceram uma relação entre *m* e *A* (a área da comunidade local):

>When an individual at location (x, y) in the local
>community dies, the replacement individual may, by virtue
>of the random dispersal and recruitment processes, be from
>within the local community (i.e., within the quadrat) or from
>outside the local community (i.e., from outside the quadrat).
>Define m x,y as the probability that the replacement individual
>at location (x, y) is drawn from outside the local community.
>This parameter will be highest for individuals on the edges
>of the quadrat and smallest for individuals at the centre of
>the quadrat, where m x,y » 0 for large A. We define m as the
>average value of m x,y across the whole of the local
>community as follows:  
  
$$eq.0:  m = \frac{1}{A} \int \int_A m_{x,y} dxdy $$

Essa equação é válida para o processo de dispersão em ambientes homogêneos, ou seja, sem fragmentação. A maneira que simulamos a dispersão em paisagens fragmentadas é diferente em uma simulação coalescente: uma vez que sorteamos um progenitor e este estaria presente em uma unidade de não habitat, o sorteio é refeito até que o progenitor esteja em uma unidade de habitat. Uma vez que o sorteio é refeito este pode cair novamente dentro da área da comunidade local, uma equação que descreve exatamente a probabilidade de um indivíduo da comunidade ser substituido por um indivíduo de fora da comunidade em uma paisagem fragmentada dependeria de explicitamente considerar a configuração espacial. Uma aproximação do efeito da fragmentação na simulação é considerar que a chance da dispersão ser oriunda de uma área de cobertura vegetal, assim, podemos corrigir m pela porcentagem de cobertura vegetal na paisagem:

$$eq.0': m' = \frac{mp}{1 - (1-p)m} $$

Onde _p_ é a porcentagem de cobertura vegetal na paisagem. É necessário corrigir o valor de m quando partimos de parâmetros da simulação coalescente em paisagens fragmentadas, contudo não é correto corrigir 'm' obtidos pelo ajuste do modelo neutro de campo médio. 

### Método C&L09 ###

A aproximação deduzida por C&L09 é $m = \frac{Pd}{\pi A}$, onde P e A são o perímetro e área do plot, respectivamente. Considerando o plot quadrado e a distribuição de Laplace podemos reescrever essa aproximação como:

<!--
m_CL <- 4 * sd_k / ( sqrt(2) * L * pi ) # eq 1a
sd_k.CL <- m * L * pi * sqrt(2) / 4 # eq 1b
-->

$$eq.1: m = sd  \frac{4}{L \pi \sqrt{2}  }  $$

Onde $P = 4 L$, $A = L^2$ e $L= 100 \sqrt{J/DA}$ metros 

### Método Coutinho apud C&L09 ###

Coutinho parte da eq.0 e aproveitando as características da simulação coalescente que utilizamos: a) utiliza apenas plot quadrados; b) a dispersão não é radial, ao invés, é descrita como o resultado do sorteio independente em eixos ortogonais. Assim, podemos escrever a eq.0 como:

$$eq.0-C.a:  m = \left(\frac{1}{L} \int\limits_{-L/2}^{L/2} m_{x}(x)\mathrm{d}x \right)^2 $$

$$eq.0-C.b: m_{x} = 1 - \int\limits_{-L/2}^{L/2} K(x-y) \mathrm{d}y  $$

K é a função de dispersão. Podemos reescrevemos considerando a distribuição de probabilidade de Laplace como:

<!--
m_CaCL <- sd_k * ( 1 -exp( -sqrt(2) * L / sd_k) ) / (sqrt(2) * L / sd_k) # eq 2a
-->
$$eq. 2a: m = sd \frac{1 - e^{-\frac{\sqrt{2} L}{sd}} }{\sqrt{2} L}$$
<!--
sd_K.CaCL <- sqrt(2) * L / (m * lambertW0(-exp(-1/m)/m) + 1) # eq 2b
-->
$$ eq. 2b: sd = \frac{\sqrt{2} L}{m W_{0}(- \frac{e^{-1/m}}{m} ) + 1} $$

Para escrever a equação em função do desvio-padrão (eq 2b) utilizamos o ramo principal da função W de Lambert ($W_0$).

### Comparação dos métodos ###

Para comparar os métodos vou utilizar os parâmetros ajustados (modelo de campor médio, por verossimilhança) à SADs amostradas na Mata Atlântica e aqueles estimados por um modelo neutro espacialmente explícito. Todos os vetores de abundância foram observados em amostras com pelo menos 1 ha. As SADs observadas foram ajustado ao modelo de campo médio e obtemos m; no modelo de espaço explícito informamos *a priori* qual o desvio padrão da função de dispersão. Então vamos utilizar as equações 1a e 2a para calcular m a partir dos desvio-padrões informados *a prior*, considerando o m' corrigido. As equações 1b e 2b irão converter m em desvio padrão. Para isso vou criar uma função que contêm as equações e a definição de $\theta$ de Hubbell (2001):

```{r convesao par, echo=T, tidy=T}
f_conv.par <- function(modelo, par., par.aux){
  #parametros que seram convertidos
  theta<- par.[[1]]
  m <- par.[[2]]
  sd_k <- par.[[3]]
  U <- par.[[4]]
  #parametros auxiliares a conversao
  p <- par.aux[[1]]
  J <- par.aux[[2]]
  DA <- par.aux[[3]]
  S <- par.aux[[4]]
  L <- 100*sqrt(J/DA)
  J_M <- 500 * p * DA
 #conversoes 
  if(modelo == "campo_medio"){ # EI -> EE
    U_1 <- theta / (2 * J_M) # Hubbell 2001
    U_2 <- theta / (J_M + theta - 1) # Etienne 2005 (qual foi a primeira referencia?)
    sd_k.CL <- m * L * pi * sqrt(2) / 4 # eq 1b
    sd_K.CaCL <- sqrt(2) * L / (m * lambertW0(-exp(-1/m)/m) + 1) # eq 2b
    df_ <- data.frame(par.value = c(U_1, U_2, sd_k.CL, sd_K.CaCL),
                      par.class = c("U","U","sd_k","sd_k"),
                      par.method = c("H01","E05","CL","CaCL"))
    return(df_)
  }else{ # EE -> EI
    theta_1 <- 2 * 500 * p * DA * U # Hubbell 2001
    theta_2 <- U * (J_M - 1) / (1 - U) # Etienne 2005
    m_CL <- 4 * sd_k / ( sqrt(2) * L * pi ) # eq 1a
    m_CaCL <- sd_k * ( 1 -exp( -sqrt(2) * L / sd_k) ) / (sqrt(2) * L / sd_k) # eq 2a
    df_ <- data.frame(par.value = c(theta_1,theta_2, m_CL, m_CaCL),
                      par.class = c("theta","theta","m","m"),
                      par.method = c("H01","E05","CL","CaCL"))
    return(df_)
  }  
}

# aplicando a formula 
df_par.conv <- ddply(df_resultados,c("SiteCode","kernel_percentil"), 
                  function(X) f_conv.par(modelo = X[,"kernel_percentil"],
                                         par. = as.list(X[,c("theta","m","sd_k","U")]),
                                         par.aux = as.list(X[,c("p","J","DA","S")])
                                         )
                    )
```

```{r inner_join df_par.conv}
# escrevendo a versão completa de df_par.conv
df_par.conv %<>% inner_join(x.,y=df_resultados[,c(1:6)], by=c("SiteCode","kernel_percentil"))
df_par.conv %<>% mutate(d = sd_k / sqrt(2),
                        d_ = sd_k_ / sqrt(2))
write.table(df_par.conv,file="/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_conv-par.txt", sep="\t")

# modificando para unidr com df_resultados
df_par.conv %<>% filter(par.method != "H01" & par.method != "CL") %>% select(-par.method) %>% 
  dcast(., ...~par.class,value.var = "par.value") %>% 
  mutate(m_ = m * p / ( 1 - (1-p)*m )) %>% select(-m)
names(df_par.conv)[c(7:9)] <- c("theta_","sd_k_","U_")

#df com os resultados de congruência, parametros estimados, parametros convertidos e parametros a posteriori
df_resultados_conv <- inner_join(x=df_resultados,
                                 y=df_par.conv,
                                 by = c("SiteCode","kernel_percentil","p","S","DA","J")) %>% 
  mutate(d = sd_k / sqrt(2),
         d_ = sd_k_ / sqrt(2),
         spp_I = - theta * (1-m) / (J * log(m)),
         spp_I_ = - theta_ * (1-m_) / (J * log(m_)),
         J_M = 500 * DA * p,
         S_M = - J_M * U * log(U) / (1-U),
         S_M_ = - J_M * U_ * log(U_) / (1-U_)
         ) %>% 
  select(SiteCode, kernel_percentil, GOF, p, J, S, DA, J_M, sd_k, sd_k_, U, U_, theta, theta_,m,m_,S_M,S_M_,spp_I,spp_I_)
names(df_resultados_conv)[2] <- "k"
levels(df_resultados_conv$k)[13] <- "EI"

#armazenando o novo df de dados.

write.table(df_resultados_conv,file="/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_resultados_conv.txt", sep="\t")
```

Esperamos que os métodos de conversão tenham valores próximos quando a relação entre L/d -> 100; d é a distância média de dispersão, que para a distribuição Laplace é igual sd/sqrt(2). 

#### m -> sd ####

Vamos avaliar a diferença entre os métodos para as conversões de 'm' para o desvio padrão da função de dispersão [EI -> EE]:

```{r avaliacao EI/EE, message=F}
# preparando dados 
df_sd_k <- df_par.conv %>% filter(kernel_percentil == "campo_medio" & par.class != "U") # %>% 
  #dcast(.,formula = SiteCode + kernel_percentil ~ par.method + par.class, value.var = "par.value")
df_sd_k %<>% inner_join(x=.,y=df_resultados[,c("SiteCode","kernel_percentil","m")],by=c("SiteCode","kernel_percentil"))

# graficos #
l_p <- vector("list",length = 4)
# sd_k ~ m * metodo
l_p[[1]] <- ggplot(df_sd_k,aes(x = m, y = par.value, colour = par.method)) + 
  geom_point() + geom_smooth(method = "loess",se=F) + 
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  labs(y="sd_k", x='m') + theme(legend.position="none") # + ggtitle(label="m -> sd_k") 
# log(sd_k) ~ m * metodo
l_p[[2]] <- ggplot(df_sd_k,aes(x = m, y = log(par.value), colour = par.method)) + 
  geom_point() + geom_smooth(method = "loess",se=F) +
  labs(y="log(sd_k)", x='m') #+ ggtitle(label="m -> log.sd_k")
# diferenca metodos ~ w/d # sd_k.CaCL - sd_k.CL ~ L / ( mean(sd_k) / sqrt(2) ) 
l_p[[3]] <- ddply(df_sd_k,c("SiteCode","J","DA"),summarise, diff.metodos = diff(par.value), mean.sd_k = mean(par.value)) %>% 
  mutate(w_d = 100 * sqrt(J/DA)/(mean.sd_k/sqrt(2))) %>% ggplot(aes(x=w_d,y=log(diff.metodos))) + geom_point() +
  scale_x_continuous(labels = function(x) format(x, scientific = TRUE)) +
  labs(x="L / ( mean(sd_k) / sqrt(2) ) ",y="log(CaCL - CL)") 
# boxplot dos valores por estimativa
l_p[[4]] <- ggplot(df_sd_k,aes(x="",y=par.value)) + geom_jitter() + geom_boxplot() + facet_wrap(~par.method,scales = "free") + labs(x="metodos",y="sd_k")
# arranjo 
grid.arrange(l_p[[1]], l_p[[2]], l_p[[3]], l_p[[4]],
             layout_matrix = rbind(c(rep(1,3),rep(2,4)),
                                   c(rep(4,3),rep(3,3),NA)) 
             )
```

figura 1. Desvio-padrão (sd_k) estimado pelas equações 1 (método CL) e 2.b (método CaCL). 1o painel = sd_k ~ m; 2o painel: log(sd_k) ~ m; 3o - boxplot de sd_k pelos métodos (título do gráfico); 4o painel: log da diferença entre os métodos e a razão entre o lado da amostra (L) e a distância média de dispersão (mean(sd_k)/sqrt(2)). mean(sd_k) é a média dos sd_k obtidos pelos dois métodos, a divição por sqrt(2) é para transformar no parâmetro escalar da distribuição de Laplace.

-Segundo a figura 1 de Chisholm & Lichstein 2009 a equação 1 deve fazer boas aproximações quando a relação entre L/d >= 100.
-O 4o painel mostra que a condição para que a equação 1 faça boas aproximações não é observada, uma vez que L/d varia em [0;2]
-O maior valor de sd_k pelo método CL ~ 300 metros, enquanto do método CaCL ~ 2e6 (valor sem realismo biológico)


#### sd -> m ####

Agora vamos avaliar a conversão de parâmetros sd_k da simulação coalescente para o respectivo m do modelo de campo médio.

```{r df_m e fig2}
# dados
df_m <- df_par.conv %>% filter(kernel_percentil != "campo_medio" & par.class != "theta")  %>%     
  inner_join(x=.,y=df_resultados[,c("SiteCode","kernel_percentil","sd_k")],by=c("SiteCode","kernel_percentil"))

#gráfico
df_m %>% ggplot(aes(x=sd_k,y=par.value, colour=par.method)) + geom_point() + geom_smooth(method = "loess",se = F) + facet_wrap(~kernel_percentil, ncol=6, scales = "free") +
  labs(x = "desvio-padrão da função de dispersão", y = "m") + ggtitle(label="sd_k -> m", subtitle = "~ %k")
```

figura 2. m ~ sd_k + %k + método de calculo. em x = desvio padrão da função de dispersão (sd_k), y = m; os paineis estão divididos pelas respectivas porcentagens de propágulos que permanecem até $l_{cel}$ metros da árvore progenitora.


```{r fig 3, fig.height=2.5,fig.width=2.5}
df_m %>% 
  mutate(w_d = 100 * sqrt(J/DA)/(sd_k/sqrt(2))) %>% 
  ddply(.,c("SiteCode","kernel_percentil","w_d"),summarise, diff.metodos = diff(par.value)) %>% 
  ggplot(aes(x=w_d,y=diff.metodos))  + geom_point() + # + geom_vline(aes(xintercept = 100, colour = "red")) +
  scale_x_continuous(labels = function(x) format(x, scientific = TRUE)) + 
  labs(x="L / (sd_k / sqrt(2) ) ",y="CaCL - CL") + theme(legend.position="none")
```

figura 3. difença no valor calculado pelos dois métodos e a razão L / (sd_k / sqrt(2) )


```{r df_m}
df_m <- df_resultados[,c(1:6,11)] %>% #par.aux + m
  filter(kernel_percentil == "campo_medio") %>% #filtrando para campo medio
  melt(.,id.vars = c("SiteCode","kernel_percentil","p","S","DA","J"),variable.name = "par.class",value.name = "par.value") %>% #melt o df
  mutate(par.method = "like") %>% # acrescentando o fator
  rbind.fill(df_m[,-10],.) %>% #concatenar
  arrange(.,SiteCode,kernel_percentil) %>% # dados em ordem
  mutate(p_class = cut(p,12)) # p_class

  #grafico
# df_m %>% ggplot(aes(x=kernel_percentil,y=par.value,shape = par.method,color = SiteCode)) + geom_point(size=2) + 
#   theme(legend.title = element_blank()) +
#   guides(color=FALSE) +
#   labs(x="%k + campo medio", y = "m") +
#   facet_wrap(~p_class,ncol=4,scales="free")
```

```{r fig 4A, fig.height=7,fig.width=10}
df_m %>% ggplot(aes(x = p, y=par.value, group=par.method)) + 
  geom_point(aes(shape=par.method,color=par.method)) + 
  geom_smooth(aes(color=par.method),method = 'loess',se=F) +
  theme(legend.title = element_blank()) +
  labs(x="p",y="m") + 
  ggtitle("m sem correção") + 
  facet_wrap(~kernel_percentil,nrow = 3,scales = "free")
```

Figura 4A. m ~ p + (~kernel_percentil)

```{r fig 4B, fig.height=7,fig.width=10}
df_m %>% 
  # mutate(m_ = ifelse(par.method != "like", par.value*p/(1-(1-p)*par.value),par.value)) %>% # eq 0'
  mutate(m_ = par.value*p/(1-(1-p)*par.value)) %>% # eq 0'
  # filter(kernel_percentil == "campo_medio") %>% mutate(m.diff = par.value - m_) %>%  ggplot(aes(x=p,y=m.diff)) + geom_point()
  ggplot(aes(x = p, y=m_, group=par.method)) + # mesma estrutura do grafico anterior
  geom_point(aes(shape=par.method,color=par.method)) + 
  geom_smooth(aes(color=par.method),method = 'loess',se=F) +
  theme(legend.title = element_blank()) +
  labs(x="p",y="m'") + 
  ggtitle("m corrigdo (eq 0')") +
  facet_wrap(~kernel_percentil,nrow = 3,scales = "free")
```

Figura 4B. m' ~ p + (~kernel_percentil)

### Theta ###

Utilizamos apenas um método para converter $\theta$ e U, a definição apresentada em Hubbell (2001): $\theta = 2 J_{M}U$. Consideramos que $J_{M} = A_{paisagem} DA p$. 

```{r df_theta e fig 5}
# dados
df_theta <- df_par.conv %>% filter(par.class == "theta") %>% # apenas os valores de theta
  dcast(.,formula = ... ~ par.class, value.var="par.value") # %>%  # transformando theta em coluna
df_theta <- df_resultados %>% filter(kernel_percentil == "campo_medio") %>% .[,c(1:6,10)] %>% mutate(par.method="like") %>% 
  rbind.fill(x=df_theta, y=.) %>% arrange(SiteCode,kernel_percentil)
  
# grafico
df_theta %>% ggplot(aes(x=p,y=theta,group=par.method,color=par.method)) + geom_point() + geom_smooth(method = 'loess',se=F) +
  scale_y_continuous(labels = function(x) format(x,scientific = TRUE)) +
  theme(legend.title = element_blank()) +
  facet_wrap(~kernel_percentil,nrow = 3,scales="free")
```

figura 5. $\theta$ ~ p + (~kernel_percentil). Cada painel corresponde ao percentil de propágulos que permanece até $l_{cel}$ metros da planta progenitora. O último painel 'campo_medio' corresponde ao $\theta$ obtido pelo ajuste da formula de Etienne (2005) às SADs observadas.

<!--
```{r figura 6}
# df_theta %>% mutate(p_class = cut(p,12)) %>% ggplot(aes(x=kernel_percentil,y=theta,group=SiteCode,shape=par.method) ) + 
#   geom_point() + geom_line() + scale_y_continuous(labels = function(x) format(x,scientific = T)) + 
#   theme(legend.position = "none") +
#   facet_grid(p_class~., scales="free")
```
figura 6. $\theta$ ~ kernel_percentil + (~p_class). p_class = cut(p,12). Os pontos e linhas estão coloridos pelo Site. 
-->

### U ###

```{r df_U e fig 7}
# dados
df_U <- df_par.conv %>% filter(par.class == "U") %>%  # apenas os valores de U
  dcast(.,formula = ... ~ par.class, value.var="par.value")  # transformando theta em coluna

df_U <- df_resultados %>% filter(kernel_percentil != "campo_medio") %>% .[,c(1:6,8)] %>% mutate(par.method="R08") %>% #Rosindell et al. 2008 
  rbind.fill(x=df_U, y=.) %>% arrange(SiteCode,kernel_percentil)

# grafico
df_U %>% ggplot(aes(x=p,y=U,group=par.method)) + geom_point(aes(color=par.method)) + geom_smooth(method = 'loess',se=F,colour="red") +
  # scale_y_continuous(labels = function(x) format(x,scientific = TRUE)) +
  theme(legend.title = element_blank()) + 
  facet_wrap(~kernel_percentil,nrow = 3,scales="free")
```

figura 7. U ~ p + (~kernel_percentil)

<!--
```{r fig 8}
# df_U %>% mutate(p_class = cut(p,12)) %>% ggplot(aes(x=kernel_percentil,y=U,group=SiteCode,colour=SiteCode)) + 
#   geom_point() + geom_line() + #scale_y_continuous(labels = function(x) format(x,scientific = T)) + 
#   theme(legend.position = "none") +
#   facet_wrap(~p_class, ncol=3,scales="free")
```

Figura 8. U ~ kernel_percentil + (~p_class); p_class = cut(p,12). Os pontos e linhas estão coloridos pelo Site.
-->

### Chuva de Propágulos ###

A chuva de propágulos pode ser entendida como o produto da fecundidade e função de dispersão (Clark et al. 1999). Por conta do pressuposto da equivalência funcional todos os indivíduos produzem o mesmo número de propágulos por unidade de tempo (Hubbell 2001) e, portanto, podemos simular cenários de limitação à dispersão em função da porcentagem de propágulos que permace até determinada distância da planta progenitora. Dessa maneira, não precisamos definir a dispersão em termos de distância per se mas em termos de porcentagem de indivíduos que permanecem na área imediata da planta progenitora. Para isso é necessário estabelecer uma distância padrão da planta progenitora e estimar ou definir a porcentagem de indivíduos que se mantêm até esta distância padrão. Como distância padronizamos $l_{cel}$, assim, cada paisagem possui uma distância padrão que depende da densidade observada de indivíduos naquela paisagem. Podemos estimar qual a porcentagem de propágulos até a distância padrão que um determinado sd gera, partindo de um m (eqn 2); ou podemos informar a priori quais as porcentagens de interesse e estimar o sd necessário para gerar tais porcentagens. Na simulação coalescente, utilizamos 12 valores de porcentagem para simular os cenários de limitação à dispersão: `r paste(c(99,seq(95,50,by=-5),25),"%")`.


Na primeira sessão estimamos o desvio-padrão da função de dispersão(sd_k) a partir do parâmetro m, estimado por verossimilhança, do modelo de campo médio. Com os valores de sd_k estimados podemos calcular a respectiva chuva de propágulos. Antes de executar essa tarefa vamos observar os valores de sd_k estimados para o modelo de espaço explícito e aqueles calculados a partir de m do campo médio pelos dois métodos de conversão:

```{r df_k_perc e fig 9, fig.height=4,fig.width=7}
#dados 
df_k_perc <- df_resultados[,1:7] %>% # sd_k EE
  filter(kernel_percentil != "campo_medio") %>% # tirando os campos_medio
  melt(.,id.vars = c("SiteCode","kernel_percentil","p","S","DA","J"), variable.name = "par.class",value.name = "par.value") %>% # melt o df
  mutate(par.method = "a_priori") %>% # acrescentando o fator
  rbind.fill(df_sd_k[,-10],.) %>% # concatenando
  arrange(.,SiteCode,kernel_percentil)

# gráfico
df_k_perc %>% ggplot(aes(x=kernel_percentil,y=log(par.value), color=par.method)) + geom_jitter() + geom_boxplot() +
  theme(legend.title = element_blank()) + 
  labs(x = "%k + campo medio", y = "log(sd_k)") +
  ggtitle(label = "Comparacao sd_k EE e EI")
```

figura 9. Boxplot log(sd_k) ~ (%k + campo medio)

- Exceto alguns valores de sd_k pelo método CL, a grande maioria de sd_k calculados para o campo médio estão em outra escala em comparação aos sd_k do modelo EE. Como pode ser visto pelo gráfico acima mas também pelos valores de m calculados a partir de sd_k do modelo EE (figura )
- Como a estimativa do percentil da chuva de propagulos (%k) é feito por função que utiliza uma abordagem numérica e não analítica, quanto maior a dispersão maior o número de pontos para se obter uma boa estimativa (?)
- Para comparar, vou calcular %k a partir dos valores de sd_k{EE} e comparar com os respectivos percentis utilizando uma regressão:

```{r k_perc, echo=T}
# funcoes
f_k_perc <- function(sigma, density, npoints=1e5){
      #metríca da simulacao e distancia de referencia
      d_ind_MA  <- 100/sqrt(density)
      # relacao entre sd e o parametro escalar da distribuicao Laplace
      b_laplace <- sigma / sqrt(2) 
      # sorteios de duas distribuicoes identicas Laplace em eixos ortogonais
      X_laplace <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA) 
      Y_laplace <- d_ind_MA * round(rlaplace(npoints, s=b_laplace) / d_ind_MA)
      #calculando a distancia dos pontos ate a origem
      dist_laplace <- sqrt(X_laplace^2+Y_laplace^2)
      #Percemtil
      percentil <- length(dist_laplace[dist_laplace<=d_ind_MA])/length(dist_laplace)
      return(percentil)
}
f_percentil.kernel <- function(i,df_=df_k_perc){
  kernel_percentil <- f_k_perc(sigma = df_[i,"par.value"],
                               density = df_[i,"DA"]
                               )
  return(kernel_percentil)
}
#armazenando
df_k_perc$k_perc <- sapply(1:nrow(df_k_perc),f_percentil.kernel)
```

Segue summary da regressão linear entre o percentil da chuva de propágulos original (k_perc0) e o estimado pela função acima (k_perc):

```{r lm k_perc}
#comparacao regressao linear
md_1 <- df_k_perc %>% filter(kernel_percentil != "campo_medio") %>% mutate(k_perc0 = as.numeric(as.character(kernel_percentil))) %>% lm(k_perc0 ~ k_perc, data=.)
summary(md_1)
```

Idealmente a regressão deveria ter $\alpha$ -> 0, $\beta$ -> 1 e $R^2$ -> 1, mas, considerando que tanto a estimativa de sd_k a partir do percentil informado (sigkernel), quanto a do percentil a partir do sd_k convertido (f_k_perc) são aproximações numéricas (anexo 1: códigos), serão considerados razoáveis e vou prosseguir com esses dados para o sd_k calculado a partir de m_{EI}:

```{r fig 11, fig.height=4}
l_p <- vector("list",length = 2) 
l_p[[1]] <- df_k_perc %>% filter(kernel_percentil == "campo_medio") %>% ggplot(aes(x=log(par.value), y=k_perc)) + 
  geom_point() +
  labs(x = "log(sd_k)", y = "prop. de propag. que permanece até l_cel metros") + 
  ggtitle("chuva de propágulos ~ [sd_k <- m_{EI}]") +
  facet_wrap(~par.method,scales="free")
l_p[[1]]
# df_k_perc[,-(3:5)]
# df_par.conv %>% filter(par.class=="m") %>% #apenas sd_k_{EE} -> m_{EE}
#   dcast(., ... ~ par.class,value.var = "par.value") %>% # Sitecode, %k, par.method, p, S, DA, J, m
#   rbind.fill(.,  # adicao dos valores para m estimado por verossimilhança (m_{EI})
#              mutate(df_resultados[df_resultados$kernel_percentil == "campo_medio",c(1:2,11)],par.method="like")) #%>% 
#   inner_join(x=.,y=df_k_perc[,-c(6:9)],by=c("SiteCode","kernel_percentil","par.method"))
# 
# df_m %>% 
#   # mutate(m_ = ifelse(par.method != "like", par.value*p/(1-(1-p)*par.value),par.value)) %>% # eq 0'
#   mutate(m_ = par.value*p/(1-(1-p)*par.value))  
#   
#   
# l_p[[2]] <- ggplot(aes(x=))
```

figura 11.

### Condit et al. 2012: calculo dos parâmetro ###

  


### Parâmetros do modelo EI a partir das SADs{EE} ###

As SAS{EE} réplicas estão armazenadas em um data frame com quatro colunas: N, rep, SiteCode, Kernel. N contêm o vetor de abundância (a SAD); rep detona qual a réplica; as outras duas colunas são de identificação com df_ad, nota: kernel = kernel_perncentil.

Então para aproveitar o código já desenvolvido: i) filtrar as SADs pelo SiteCode; ii) transformar em lista de SADs

SdT: i) ajustar 'fitetienne' a todas as SADs{EE} réplicas para cada conjunto de parâmetros para cada Site e concatenar tudo em um data frame 


```{r}
## Qual a estrutura dos dados? (SADs{EE})##
load("/home/danilo/Documentos/Doutorado/dados/l_dados_brutos.Rdata")
df_SAD.sim <- l_dados_brutos$df_SAD.sim %>% filter(SiteCode %in% SiteCodes) %>% droplevels()
```

-> Ajustando as SADs{EE}
obs: leva um bom tempo para fazer todo o processo; estimei mais de 10 horas usando os 4 processadores do meu computador e usando a pari/GP na função fitetienne 
```{r}
# Estimando os parâmetros a partir da lista de SADs e armazenar os valores em um data frame #
registerDoMC(4)
df_EE_viaEI <- ddply(.data=df_SAD.sim, .variables = c("SiteCode","kernel","rep") , .fun=fitetienne, .parallel = TRUE)
write.table(df_EE_viaEI,file = "/home/danilo/Documentos/Doutorado/artigo_mestrado/Rmd_e_dados/df_SAD_EE-parEI-viaEI.txt",sep = "\t",row.names = FALSE)
```

alguns gráficos exploratórios

```{r}
df_EE_viaEI %>% ggplot(aes(x=SiteCode,y=theta)) + geom_boxplot() + facet_wrap(~kernel,nrow = 3)
df_temp %>% ggplot(aes(x=SiteCode,y=theta_var)) + geom_boxplot() + facet_wrap(~kernel,nrow = 3)
```


